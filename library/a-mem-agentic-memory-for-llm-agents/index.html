<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>A-MEM: Agentic Memory for LLM Agents | Content</title></head><body><nav style=margin-bottom:2em><a href=/>← Home</a></nav><main><article><h1>A-MEM: Agentic Memory for LLM Agents</h1><h2 id=a-mem-agentic-memory-for-llm-agents>A-MEM: Agentic Memory for LLM Agents</h2><p><strong>Authors:</strong> Wujiang Xu, Zuzie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang</p><p><strong>Affiliations:</strong> Rutgers University, AntGroup, Salesforce Research</p><h3 id=abstract>Abstract</h3><p>While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems&rsquo; fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way.</p><p>Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution—as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding.</p><p>Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at <a href=https://github.com/WujiangXu/AgenticMemory>https://github.com/WujiangXu/AgenticMemory</a>.</p><h2 id=1-introduction>1. Introduction</h2><p>Large Language Model (LLM) agents have demonstrated remarkable capabilities in various tasks, with recent advances enabling them to interact with environments, execute tasks, and make decisions autonomously. They integrate LLMs with external tools and delicate workflows to improve reasoning and planning abilities. Though LLM agents have strong reasoning performance, they still need a memory system to provide long-term interaction ability with the external environment.</p><p>Existing memory systems for LLM agents provide basic memory storage functionality. These systems require agent developers to predefine memory storage structures, specify storage points within the workflow, and establish retrieval timing. Meanwhile, to improve structured memory organization, Mem0, following the principles of RAG (Retrieval-Augmented Generation), incorporates graph databases for storage and retrieval processes. While graph databases provide structured organization for memory systems, their reliance on predefined schemas and relationships fundamentally limits their adaptability. This limitation manifests clearly in practical scenarios—when an agent learns a novel mathematical solution, current systems can only categorize and link this information within their preset framework, unable to forge innovative connections or develop new organizational patterns as knowledge evolves. Such rigid structures, coupled with fixed agent workflows, severely restrict these systems&rsquo; ability to generalize across new environments and maintain effectiveness in long-term interactions. The challenge becomes increasingly critical as LLM agents tackle more complex, open-ended tasks, where flexible knowledge organization and continuous adaptation are essential. Therefore, how to design a flexible and universal memory system that supports LLM agents&rsquo; long-term interactions remains a crucial challenge.</p><p>In this paper, we introduce a novel agentic memory system, named A-MEM, for LLM agents that enables dynamic memory structuring without relying on static, predetermined memory operations. Our approach draws inspiration from the Zettelkasten method, a sophisticated knowledge management system that creates interconnected information networks through atomic notes and flexible linking mechanisms. Our system introduces an agentic memory architecture that enables autonomous and flexible memory management for LLM agents.</p><p>For each new memory, we construct comprehensive notes, which integrates multiple representations: structured textual attributes including several attributes and embedding vectors for similarity matching. Then A-MEM analyzes the historical memory repository to establish meaningful connections based on semantic similarities and shared attributes. This integration process not only creates new links but also enables dynamic evolution when new memories are incorporated, they can trigger updates to the contextual representations of existing memories, allowing the entire memories to continuously refine and deepen its understanding over time. The contributions are summarized as:</p><ul><li><p><strong>We present A-MEM</strong>, an agentic memory system for LLM agents that enables autonomous generation of contextual descriptions, dynamic establishment of memory connections, and intelligent evolution of existing memories based on new experiences. This system equips LLM agents with long-term interaction capabilities without requiring predetermined memory operations.</p></li><li><p><strong>We design an agentic memory update mechanism</strong> where new memories automatically trigger two key operations: (1) <strong>Link Generation</strong>—automatically establishing connections between memories by identifying shared attributes and similar contextual descriptions, and (2) <strong>Memory Evolution</strong>—enabling existing memories to dynamically evolve as new experiences are analyzed, leading to the emergence of higher-order patterns and attributes.</p></li><li><p><strong>We conduct comprehensive evaluations</strong> of our system using a long-term conversational dataset, comparing performance across six foundation models using six distinct evaluation metrics, demonstrating significant improvements. Moreover, we provide T-SNE visualizations to illustrate the structured organization of our agentic memory system.</p></li></ul><h2 id=2-related-work>2. Related Work</h2><h3 id=21-memory-for-llm-agents>2.1 Memory for LLM Agents</h3><p>Prior works on LLM agent memory systems have explored various mechanisms for memory management and utilization. Some approaches complete interaction storage, which maintains comprehensive historical records through dense retrieval models or read-write memory structures. Moreover, MemGPT leverages cache-like architectures to prioritize recent information. Similarly, SCM (Self-Controlled Memory) proposes a Self-Controlled Memory framework that enhances LLMs&rsquo; capability to maintain long-term memory through a memory stream and controller mechanism. However, these approaches face significant limitations in handling diverse real-world tasks. While they can provide basic memory functionality, their operations are typically constrained by predefined structures and fixed workflows. These constraints stem from their reliance on rigid operational patterns, particularly in memory writing and retrieval processes. Such inflexibility leads to poor generalization in new environments and limited effectiveness in long-term interactions. Therefore, designing a flexible and universal memory system that supports agents&rsquo; long-term interactions remains a crucial challenge.</p><h3 id=22-retrieval-augmented-generation>2.2 Retrieval-Augmented Generation</h3><p>Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to enhance LLMs by incorporating external knowledge sources. The standard RAG process involves indexing documents into chunks, retrieving relevant chunks based on semantic similarity, and augmenting the LLM&rsquo;s prompt with this retrieved context for generation. Advanced RAG systems have evolved to include sophisticated pre-retrieval and post-retrieval optimizations.</p><p>Building upon these foundations, recent research has introduced agentic RAG systems that demonstrate more autonomous and adaptive behaviors in the retrieval process. These systems can dynamically determine when and what to retrieve, generate hypothetical responses to guide retrieval, and iteratively refine their search strategies based on intermediate results.</p><p>However, while agentic RAG approaches demonstrate agency in the retrieval phase by autonomously deciding when and what to retrieve, our agentic memory system exhibits agency at a more fundamental level through the autonomous evolution of its memory structure. Inspired by the Zettelkasten method, our system allows memories to actively generate their own contextual descriptions, form meaningful connections with related memories, and evolve both their content and relationships as new experiences emerge. This fundamental distinction in agency between retrieval versus storage and evolution distinguishes our approach from agentic RAG systems, which maintain static knowledge bases despite their sophisticated retrieval mechanisms.</p><h2 id=3-methodology>3. Methodology</h2><p>Our proposed agentic memory system draws inspiration from the Zettelkasten method, implementing a dynamic and self-evolving memory system that enables LLM agents to maintain long-term memory without predetermined operations. The system&rsquo;s design emphasizes atomic note-taking, flexible linking mechanisms, and continuous evolution of knowledge structures.</p><h3 id=31-note-construction>3.1 Note Construction</h3><p>Building upon the Zettelkasten method&rsquo;s principles of atomic note-taking and flexible organization, we introduce an LLM-driven approach to memory note construction. When an agent interacts with its environment, we construct structured memory notes that capture both explicit information and LLM-generated contextual understanding. Each memory note in our collection M = {m₁, m₂, &mldr;, m_N} is represented as:</p><p>m_i = {c_i, t_i, K_i, G_i, X_i, e_i, L_i}</p><p>where c_i represents the original interaction content, t_i is the timestamp of the interaction, K_i denotes LLM-generated keywords that capture key concepts, G_i contains LLM-generated tags for categorization, X_i represents the LLM-generated contextual description that provides rich semantic understanding, and L_i maintains the set of linked memories that share semantic relationships. To enrich each memory note with meaningful context beyond its basic content and timestamp, we leverage an LLM to analyze the interaction and generate these semantic components. The note construction process involves prompting the LLM with carefully designed templates:</p><p>K_i, G_i, X_i ← LLM(c_i ∥ t_i ∥ P_s1)</p><p>Following the Zettelkasten principle of atomicity, each note captures a single, self-contained unit of knowledge. To enable efficient retrieval and linking, we compute a dense vector representation via a text encoder that encapsulates all textual components of the note:</p><p>e_i = f_enc[concat(c_i, K_i, G_i, X_i)]</p><p>By using LLMs to generate enriched components, we enable autonomous extraction of implicit knowledge from raw interactions. The multi-faceted note structure (K_i, G_i, X_i) creates rich representations that capture different aspects of the memory, facilitating nuanced organization and retrieval. Additionally, the combination of LLM-generated semantic components with dense vector representations provides both human-interpretable context and computationally efficient similarity matching.</p><h3 id=32-link-generation>3.2 Link Generation</h3><p>Our system implements an autonomous link generation mechanism that enables new memory notes to form meaningful connections without predefined rules. When the constructed memory note m_n is added to the system, we first leverage its semantic embedding for similarity-based retrieval. For each existing memory note m_j ∈ M, we compute a similarity score:</p><p>s_n,j = (e_n · e_j) / (|e_n| |e_j|)</p><p>The system then identifies the top-k most relevant memories:</p><p>M_near = {m_j | rank(s_n,j) ≤ k, m_j ∈ M}</p><p>Based on these candidate nearest memories, we prompt the LLM to analyze potential connections based on their potential common attributes. Formally, the link set of memory m_n updates like:</p><p>L_i ← LLM(m_n ∥ M_near ∥ P_s2)</p><p>Each generated link l is structured as: L_i = {m_i, &mldr;, m_k}. By using embedding-based retrieval as an initial filter, we enable efficient scalability while maintaining semantic relevance. A-MEM can quickly identify potential connections even in large memory collections without exhaustive comparison. More importantly, the LLM-driven analysis allows for nuanced understanding of relationships that goes beyond simple similarity metrics. The language model can identify subtle patterns, causal relationships, and conceptual connections that might not be apparent from embeddings similarity alone. We implement the Zettelkasten principle of flexible linking while leveraging modern language models. The resulting network emerges organically from memory content and context, enabling natural knowledge organization.</p><h3 id=33-memory-evolution>3.3 Memory Evolution</h3><p>After creating links for the new memory, A-MEM evolves the retrieved memories based on their textual information and relationships with the new memory. For each memory m_j in the nearest neighbor set M_near, the system determines whether to update its context, keywords, and tags. This evolution process can be formally expressed as:</p><p>m*_j ← LLM(m_n ∥ M_near \ m_j ∥ m_j ∥ P_s3)</p><p>The evolved memory m*_j then replaces the original memory m_j in the memory set M. This evolutionary approach enables continuous updates and new connections, mimicking human learning processes. As the system processes more memories over time, it develops increasingly sophisticated knowledge structures, discovering higher-order patterns and concepts across multiple memories. This creates a foundation for autonomous memory learning where knowledge organization becomes progressively richer through the ongoing interaction between new experiences and existing memories.</p><h3 id=34-retrieve-relative-memory>3.4 Retrieve Relative Memory</h3><p>In each interaction, our A-MEM performs context-aware memory retrieval to provide the agent with relevant historical information. Given a query text q from the current interaction, we first compute its dense vector representation using the same text encoder used for memory notes:</p><p>e_q = f_enc(q)</p><p>The system then computes similarity scores between the query embedding and all existing memory notes in M using cosine similarity:</p><p>s_q,i = (e_q · e_i) / (|e_q| |e_i|), where e_i ∈ m_i, ∀m_i ∈ M</p><p>Then we retrieve the k most relevant memories from the historical memory storage to construct a contextually appropriate prompt.</p><p>M_retrieved = {m_i | rank(s_q,i) ≤ k, m_i ∈ M}</p><p>These retrieved memories provide relevant historical context that helps the agent better understand and respond to the current interaction. The retrieved context enriches the agent&rsquo;s reasoning process by connecting the current interaction with related past experiences and knowledge stored in the memory system.</p><h2 id=4-experiment>4. Experiment</h2><h3 id=41-dataset-and-evaluation>4.1 Dataset and Evaluation</h3><p>To evaluate the effectiveness of instruction-aware recommendation in long-term conversations, we utilize the LoCoMo dataset, which contains significantly longer dialogues compared to existing conversational datasets. While previous datasets contain dialogues with around 1K tokens over 4-5 sessions, LoCoMo features much longer conversations averaging 9K tokens spanning up to 35 sessions, making it particularly suitable for evaluating models&rsquo; ability to handle long-range dependencies and maintain consistency over extended conversations. The LoCoMo dataset comprises diverse question types designed to comprehensively evaluate different aspects of model understanding:</p><ol><li><strong>Single-hop questions</strong> answerable from a single session</li><li><strong>Multi-hop questions</strong> requiring information synthesis across sessions</li><li><strong>Temporal reasoning questions</strong> testing understanding of time-related information</li><li><strong>Open-domain knowledge questions</strong> requiring integration of conversation context with external knowledge</li><li><strong>Adversarial questions</strong> assessing models&rsquo; ability to identify unanswerable queries</li></ol><p>In total, LoCoMo contains 7,512 question-answer pairs across these categories.</p><p>For evaluation, we employ two primary metrics:</p><ul><li><strong>F1 score</strong> to assess answer accuracy by balancing precision and recall</li><li><strong>BLEU-1</strong> to evaluate generated response quality by measuring word overlap with ground truth responses</li></ul><p>Also, we report the average token length for answering one question. Besides, we report the experimental results with four extra metrics including ROUGE-L, ROUGE-2, METEOR and SBERT Similarity in the Appendix.</p><h3 id=42-implementation-details>4.2 Implementation Details</h3><p>For all baselines and our proposed method, we maintain consistency by employing identical system prompts as detailed in Appendix C. The deployment of Qwen-1.5B/3B and Llama3.2-1B/3B models is accomplished through local instantiation using Ollama, with LiteLLM managing structured output generation. For GPT models, we utilize the official structured output API. In our memory retrieval process, we primarily employ k=10 for top-k memory selection to maintain computational efficiency, while adjusting this parameter for specific categories to optimize performance. The detailed configurations of k can be found in Appendix B.4. For text embedding, we implement the all-minilm-l6-v2 model across all experiments.</p><h3 id=43-baselines>4.3 Baselines</h3><p><strong>LoCoMo</strong> takes a direct approach by leveraging foundation models without memory mechanisms for question answering tasks. For each query, it incorporates the complete preceding conversation and questions into the prompt, evaluating the model&rsquo;s reasoning capabilities.</p><p><strong>ReadAgent</strong> tackles long-context document processing through a sophisticated three-step methodology: it begins with episode pagination to segment content into manageable chunks, followed by memory gisting to distill each page into concise memory representations, and concludes with interactive look-up to retrieve pertinent information as needed.</p><p><strong>MemoryBank</strong> introduces an innovative memory management system that maintains and efficiently retrieves historical interactions. The system features a dynamic memory updating mechanism based on the Ebbinghaus Forgetting Curve theory, which intelligently adjusts memory strength according to time and significance. Additionally, it incorporates a user portrait building system that progressively refines its understanding of user personality through continuous interaction analysis.</p><p><strong>MemGPT</strong> presents a novel virtual context management system drawing inspiration from traditional operating systems&rsquo; memory hierarchies. The architecture implements a dual-tier structure: a main context (analogous to RAM) that provides immediate access during LLM inference, and an external context (analogous to disk storage) that maintains information beyond the fixed context window.</p><h3 id=44-empirical-results>4.4 Empirical Results</h3><p>In our empirical evaluation, we compared A-MEM with four competitive baselines including LoCoMo, ReadAgent, MemoryBank, and MemGPT on the LoCoMo dataset. For non-GPT foundation models, our A-MEM consistently outperforms all baselines across different categories, demonstrating the effectiveness of our agentic memory approach. For GPT-based models, while LoCoMo and MemGPT show strong performance in certain categories like Open Domain and Adversarial tasks due to their robust pre-trained knowledge in simple fact retrieval, our A-MEM demonstrates superior performance in Multi-Hop tasks achieving at least two times better performance that require complex reasoning chains.</p><p>The effectiveness of A-MEM stems from its novel agentic memory architecture that enables dynamic and structured memory management. Unlike traditional approaches that use static memory operations, our system creates interconnected memory networks through atomic notes with rich contextual descriptions, enabling more effective multi-hop reasoning. The system&rsquo;s ability to dynamically establish connections between memories based on shared attributes and continuously update existing memory descriptions with new contextual information allows it to better capture and utilize the relationships between different pieces of information.</p><p>Notably, A-MEM achieves these improvements while maintaining significantly lower token length requirements compared to LoCoMo and MemGPT (around 1,200-2,500 tokens versus 16,900 tokens) through our selective top-k retrieval mechanism. In conclusion, our empirical results demonstrate that A-MEM successfully combines structured memory organization with dynamic memory evolution, leading to superior performance in complex reasoning tasks while maintaining computational efficiency.</p><h3 id=45-ablation-study>4.5 Ablation Study</h3><p>To evaluate the effectiveness of the Link Generation (LG) and Memory Evolution (ME) modules, we conduct the ablation study by systematically removing key components of our model. When both LG and ME modules are removed, the system exhibits substantial performance degradation, particularly in Multi Hop reasoning and Open Domain tasks.</p><p>The system with only LG active (w/o ME) shows intermediate performance levels, maintaining significantly better results than the version without both modules, which demonstrates the fundamental importance of link generation in establishing memory connections. Our full model, A-MEM, consistently achieves the best performance across all evaluation categories, with particularly strong results in complex reasoning tasks. These results reveal that while the link generation module serves as a critical foundation for memory organization, the memory evolution module provides essential refinements to the memory structure. The ablation study validates our architectural design choices and highlights the complementary nature of these two modules in creating an effective memory system.</p><h3 id=46-hyperparameter-analysis>4.6 Hyperparameter Analysis</h3><p>We conducted extensive experiments to analyze the impact of the memory retrieval parameter k, which controls the number of relevant memories retrieved for each interaction. As shown in Figure 3, we evaluated performance across different k values (10, 20, 30, 40, 50) on five categories of tasks using GPT-4-mini as our base model. The results reveal an interesting pattern: while increasing k generally leads to improved performance, this improvement gradually plateaus and sometimes slightly decreases at higher values. This trend is particularly evident in Multi Hop and Open Domain tasks.</p><p>The observation suggests a delicate balance in memory retrieval—while larger k values provide richer historical context for reasoning, they may also introduce noise and challenge the model&rsquo;s capacity to process longer sequences effectively. Our analysis indicates that moderate k values strike an optimal balance between context richness and information processing efficiency.</p><h3 id=47-memory-analysis>4.7 Memory Analysis</h3><p>We present the t-SNE visualization of memory embeddings to demonstrate the structural advantages of our agentic memory system. Analyzing two dialogues sampled from long-term conversations in LoCoMo, we observe that A-MEM (shown in blue) consistently exhibits more coherent clustering patterns compared to the baseline system (shown in red). This structural organization is particularly evident in Dialogue 2, where well-defined clusters emerge in the central region, providing empirical evidence for the effectiveness of our memory evolution mechanism and contextual description generation.</p><p>In contrast, the baseline memory embeddings display a more dispersed distribution, demonstrating that memories lack structural organization without our link generation and memory evolution components. These visualization results validate that A-MEM can autonomously maintain meaningful memory structures through dynamic evolution and linking mechanisms. More results can be seen in Appendix B.3.</p><h2 id=5-conclusion>5. Conclusion</h2><p>In this work, we introduced A-MEM, a novel agentic memory system that enables LLM agents to dynamically organize and evolve their memories without relying on predefined structures. Drawing inspiration from the Zettelkasten method, our system creates an interconnected knowledge network through dynamic indexing and linking mechanisms that adapt to diverse real-world tasks. The system&rsquo;s core architecture features autonomous generation of contextual descriptions for new memories and intelligent establishment of connections with existing memories based on shared attributes. Furthermore, our approach enables continuous evolution of historical memories by incorporating new experiences and developing higher-order attributes through ongoing interactions.</p><p>Through extensive empirical evaluation across six foundation models, we demonstrated that A-MEM achieves superior performance compared to existing state-of-the-art baselines in long-term conversational tasks. Visualization analysis further validates the effectiveness of our memory organization approach. These results suggest that agentic memory systems can significantly enhance LLM agents&rsquo; ability to utilize long-term knowledge in complex environments.</p><h2 id=6-limitation>6. Limitation</h2><p>While our agentic memory system achieves promising results, we acknowledge several areas for potential future exploration. First, although our system dynamically organizes memories, the quality of these organizations may still be influenced by the inherent capabilities of the underlying language models. Different LLMs might generate slightly different contextual descriptions or establish varying connections between memories. Additionally, while our current implementation focuses on text-based interactions, future work could explore extending the system to handle multimodal information, such as images or audio, which could provide richer contextual representations.</p><hr><p><a href=footnotes/>View Footnotes</a> | <a href=/>Back to Library</a></p><section><h2>Contents</h2><ul><li><a href=/library/a-mem-agentic-memory-for-llm-agents/footnotes/>Footnotes - A-MEM: Agentic Memory for LLM Agents</a></li></ul></section></article></main></body></html>