<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Footnotes - Serverless Computing: One Step Forward, Two Steps Back | Content</title></head><body><nav style=margin-bottom:2em><a href=/>← Library Index</a></nav><main><article><h1 id=footnotes>Footnotes</h1><p><a id=fn1></a><strong>[1]</strong> This might be better termed a &ldquo;proprietary library&rdquo;, but the analogy to C&rsquo;s stdlib is apropos: not officially part of the programming model, but integral in practice. <a href=#fnref1>↩</a></p><p><a id=fn2></a><strong>[2]</strong> In many blog posts ostensibly about serverless computing, FaaS is combined with &ldquo;non-serverless&rdquo; services: i.e., services that do not autoscale, like AWS Elasticache. Design patterns that use non-serverless services are out of scope of our discussion; one might even argue they are anti-patterns for serverless development since they do not autoscale. <a href=#fnref2>↩</a></p><p><a id=fn3></a><strong>[3]</strong> Our discussion in this paper represents the state of AWS Lambda as of Fall 2018. Some new details were announced at the AWS re:Invent conference in late November 2018. We have only had time before publication deadline to comment on them briefly where they may affect our reported results. These announcements do not seem to address our main concerns in a substantive way. <a href=#fnref3>↩</a></p><p><a id=fn4></a><strong>[4]</strong> AWS announced the availability of 100 Gbps networking on 11/26/2018. This moves the needle but leaves the problem unsolved: once you hit the cap, you are constrained. Even with 100 Gbps/64 cores, under load you get ∼200 MBps per core, still an order of magnitude slower than a single SSD. <a href=#fnref4>↩</a></p><p><a id=fn5></a><strong>[5]</strong> On 11/26/2018 AWS announced the availability of Firecracker, a microVM framework that supports 125 ms startup time for vanilla VMs. This would have at best modest effects on our results in Table 1; it is still orders of magnitude slower than traditional network messaging. <a href=#fnref5>↩</a></p><p><a id=fn6></a><strong>[6]</strong> In &ldquo;election mode&rdquo;—when leader election is happening—each node does about 10 reads every time it polls the storage medium, which happens 4 times a second. In non-election mode, each node does 2 reads per polling cycle. Our cost estimate represents the best case scenario, in which each leader is elected immediately after joining the cluster—in practice, costs might be much higher. <a href=#fnref6>↩</a></p><hr><p><a href=../>Back to Main Content</a> | <a href=/>Back to Library</a></p></article></main></body></html>